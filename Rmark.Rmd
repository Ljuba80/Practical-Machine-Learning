---
title: "HAR Analysis Using R"
author: "ljuba80"
output:
  html_document:
    fig_width: 11
    keep_md: yes
    theme: readable
    toc: yes
---
###Syllabus
This report is part of course project in Practical Machine Learning.Data was divided in two parts - Training(70%) and test data (30%). Because of performance
issues, random forest training function applied. No crossvalidation is needed in 
this case because it is performed internally. We are expecting overall accuracy above 95%
###Data Processing
First we load data
```{r,echo=TRUE}
urlTrainData<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv?accessType=DOWNLOAD"
trainData=read.csv(urlTrainData,header=TRUE,na.strings = "NA")
urlTestData<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv?accessType=DOWNLOAD"
testData=read.csv(urlTestData,header=TRUE,na.strings = "NA")
```


###Imputting missing values

As there is number of missing values, we must impute them. Most obvious choice is
to use mean value of appropriate data. Also, factor variables with float levels are transformed to numeric variables
```{r,echo=TRUE}
for(i in names(trainData))
{
  missingIdx<-is.na(trainData[,i]) 
  if(class(trainData[,i]) == "integer")
  {
    valToImput<-mean(trainData[,i],na.rm=TRUE)
    trainData[missingIdx,i]<-round(valToImput)
  }
  else if(class(trainData[,i]) == "numeric")
  {
    valToImput<-mean(trainData[,i],na.rm=TRUE)
    trainData[missingIdx,i]<-valToImput
  }
  else if (class(trainData[,i]) == "factor")
  {
    if(length(levels(trainData[,i]))>32)
    {
      trainData[,i]<-as.numeric(trainData[,i])
    }  
  }
}

for(i in names(testData))
{
  missingIdx<-is.na(testData[,i]) 
  if(class(testData[,i]) == "integer")
  {
    valToImput<-mean(testData[,i],na.rm=TRUE)
    testData[missingIdx,i]<-round(valToImput)
  }
  else if(class(testData[,i]) == "numeric")
  {
    valToImput<-mean(testData[,i],na.rm=TRUE)
    testData[missingIdx,i]<-valToImput
  }
  else if (class(testData[,i]) == "factor")
  {
    if(length(levels(testData[,i]))>32)
    {
      testData[,i]<-as.numeric(testData[,i])
    } 
  }  
}
```

Now we partition our (train) dataset in two parts (trained and test part).Also, first seven variables are removed from training set.j
```{r,echo=TRUE}
library(caret)
library(randomForest)
set.seed(32323)
inTrain <- createDataPartition(y = trainData$classe, p = 0.7, list = FALSE)
training <- trainData[inTrain, ]
testing <- trainData[-inTrain, ]
p <- training[, -1:-7]
model.treebag <- randomForest(classe ~ ., data = p,ntree=500)
result2 <- predict(model.treebag, newdata = testing)
confusionMatrix(result2, testing$classe)
```